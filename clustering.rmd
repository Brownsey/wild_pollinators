---
title: "clustering"
author: "Stephen Brownsey"
date: "25/12/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, results= "hide", message=F, warning=F, echo = FALSE}
library(tidyverse)
library(GGally)
library(gridExtra)
library(class)
library(cluster)
library(clValid)
library(modeest)
library(broom)
```

### 2 step markov chain I guess
This section is going cover two step clustering based on various clustering methods for comparison.
Visit data will be combined into one bee result for each orchard, note in principle this is fundementally flawed as the data does depend on previous years and the assumption that independent of the past is false as pesticide last year affects bee population this year as would be expected. In these scenarios each implementation will have a maximum of 8 clusters. Two after pre-bloom, 4 after and 8 after post bloom.
```{r}
load("data")
markov_data <- data_2012 %>%
  select(ends_with(".pre"), ends_with(".blm"), ends_with(".pos"), orchard) %>%
  unique()

#function to generate the agglomaerative clustering
aggl <- function(data){
  agnes(dist(data, method = "euclidian"), 
                   diss=TRUE, method = "ward")
}

#function to reduce copy pasting for each node
aggl_c <- function(data, ends, c_num){
  #define temp dataset of this stage by cluster
    temp <- data %>%
    filter(cluster == c_num) %>%
    select(ends_with(ends))
    
#If statement as if 1 element in dataset it'll error    
if(nrow(temp) > 1){
  temp$cluster <- cutree(aggl(temp) , k = 2)
  
   output <- data %>% 
    filter(cluster == c_num) %>%
    select(-cluster) %>%
    mutate(cluster = temp$cluster)
   #Else statement for if 1 element in dataset
  }else{
    #If only 1 element in the dataset -> set the cluster number to 1
    output <- data %>% 
      filter(cluster == c_num) %>%
      select(-cluster) %>%
      mutate(cluster = 1)
  }
  output
}


####Agglomerative heirachicale clustering
#pre-bloom step
temp <- markov_data %>%
  select(ends_with(".pre"))
  
temp$cluster <- cutree(aggl(temp) , k = 2)

aggl_node1 <- markov_data %>% 
  mutate(cluster = temp$cluster)

#during bloom step 1
aggl_node2 <- aggl_c(aggl_node1, ".blm", 1)
  
#during bloom step 2
aggl_node3 <- aggl_c(aggl_node1, ".blm", 2)


#post bloom step 1
aggl_node4 <- aggl_c(aggl_node2, ".pos", 1)
  
#post bloom step 2
aggl_node5 <- aggl_c(aggl_node2, ".pos", 2)

#post bloom step 3
aggl_node6 <- aggl_c(aggl_node3, ".pos", 1)
  
#post bloom step 4
#Note this node only has 1 element in it so it is automatically set to 1
aggl_node7 <- aggl_c(aggl_node3, ".pos", 2)

orchard_node_agglom <- tibble(orchard = aggl_node4 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 8) %>%
  bind_rows(tibble(orchard = aggl_node4 %>% filter(cluster == 2) %>% select(orchard) %>% as_vector, node = 9 )) %>%
  bind_rows(tibble(orchard = aggl_node5 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 10)) %>%
  bind_rows(tibble(orchard = aggl_node5 %>% filter(cluster == 2) %>% select(orchard) %>% as_vector, node = 11)) %>%
  bind_rows(tibble(orchard = aggl_node6 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 12)) %>%
  bind_rows(tibble(orchard = aggl_node6 %>% filter(cluster == 2) %>% select(orchard) %>% as_vector, node = 13)) %>%
  bind_rows(tibble(orchard = aggl_node7 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 14))
```

kmeans approach: Even running the 1001 times it is different everytime: store output later on but run it like 1million times at each stage.

```{r}
#function to generate the kmeans clustering
k_clustering <- function(data, n = 1001){
#creating the dataset to then calculate the optimal cluster from  
  for(i in 1:n){
  if(i == 1){
     k_list <- tibble(kmeans(data, 2)$cluster)
  }else{
     k_list <- bind_cols(k_list, tibble(kmeans(data, 2)$cluster))
  }
  
  }
  apply(k_list[ ,1:length(k_list)], 1, mfv1) %>% 
    enframe(value = "cluster") %>%
    select(cluster)
}
#pre-bloom step
#Getting the most common clustering 

kmeans_c <- function(data, ends, c_num){
  
  data <- data %>%
    filter(cluster == c_num) %>%
    select(-cluster)
  
  if(nrow(data) > 2){
    temp <- data  %>%
    select(ends_with(ends))
  
  cluster <- k_clustering(temp, 2)
 output <- bind_cols(data, cluster)
  }else{
    #Not more than 2 rows and get the error message as:
    #number of cluster centres must lie between 1 and nrow(x)
    output <- data %>%
      mutate(cluster = 1)
  }

 output
}


temp <- k_clustering(markov_data %>% select(-orchard))

kmeans_node1 <- markov_data %>% 
  bind_cols(temp)


#during bloom step 1
kmeans_node2 <- kmeans_c(kmeans_node1, ".blm", 1)
  
#during bloom step 2
kmeans_node3 <- kmeans_c(kmeans_node1, ".blm", 2)


#post bloom step 1
kmeans_node4 <- kmeans_c(kmeans_node2, ".pos", 1)
  
#post bloom step 2
kmeans_node5 <- kmeans_c(kmeans_node2, ".pos", 2)

#post bloom step 3
kmeans_node6 <- kmeans_c(kmeans_node3, ".pos", 1)
  
#post bloom step 4
kmeans_node7 <- kmeans_c(kmeans_node3, ".pos", 2)

orchard_node_kmeans <- tibble(orchard = kmeans_node4 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 8) %>%
  bind_rows(tibble(orchard = kmeans_node4 %>% filter(cluster == 2) %>% select(orchard) %>% as_vector, node = 9 )) %>%
  bind_rows(tibble(orchard = kmeans_node5 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 10)) %>%
  bind_rows(tibble(orchard = kmeans_node5 %>% filter(cluster == 2) %>% select(orchard) %>% as_vector, node = 11)) %>%
  bind_rows(tibble(orchard = kmeans_node6 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 12)) %>%
  bind_rows(tibble(orchard = kmeans_node6 %>% filter(cluster == 2) %>% select(orchard) %>% as_vector, node = 13)) %>%                  bind_rows(tibble(orchard = kmeans_node7 %>% filter(cluster == 1) %>% select(orchard) %>% as_vector, node = 14)) %>%
  bind_rows(tibble(orchard = kmeans_node7 %>% filter(cluster == 2) %>% select(orchard) %>% as_vector, node = 15))
```

Just a look at the difference in the final clusters between the two methods
```{r}
clusterings <- orchard_node_agglom %>%
  arrange(orchard) %>%
  bind_cols(node_km = orchard_node_kmeans %>% arrange(orchard) %>%
              select(node) %>% as_vector)
clusterings

```

## Updating the bee values to match the environments
In this particular analysis, the interest is in the affects of pesticides on bee population. As such, it is necessary to correct, as best we can, for extra factors (confounding?) to make the bee values representative.

All these graphs were plotted as part of EDA by the use of lapply to the a basic plotting function. Here the relationships between some of the main extra factors (are they confounding?) will be examined.

##### Temperature
It is known that temperature affects the speed at which bees fly, as such this will have an effect on bee abundance and possibly richness although more bees might not necessarily mean more species are observed. From the previous analysis carried out it is known that a log(x) + 1 transposition of bee count allows for a "better" model fit and as such in general this will the case for our observations.
```{r}
##agglom
wild_bee_abundance_agglom <- data_2012 %>%
  inner_join(orchard_node_agglom) %>%
  ggplot(aes(x = temp, y = log(wildAbF + 1))) +
  geom_point(aes(colour = as_factor(node))) +
  geom_smooth() +
  geom_smooth(method = "lm", colour = "red") +
  labs(x = "Temperature", y = "Wild Bee Abundance (log(x) + 1)",
       title = "Wild Bee Abundance Coloured by Agglomerature Nodes",
       colour = "Agglomerative clustering nodes") +
  theme_bw() 
wild_bee_abundance_agglom  

##kmeans
wild_bee_abundance_kmeans <- data_2012 %>%
  inner_join(orchard_node_kmeans) %>%
  ggplot(aes(x = temp, y = log(wildAbF + 1))) +
  geom_point(aes(colour = as_factor(node))) +
  geom_smooth() +
  geom_smooth(method = "lm", colour = "red") +
  labs(x = "Temperature", y = "Wild Bee Abundance (log(x) + 1)",
       title = "Wild Bee Abundance Coloured by Kmeans Nodes",
       colour = "Kmeans clustering nodes") +
  theme_bw() 
wild_bee_abundance_kmeans  

##Linear model
lm_temp <- lm(log(data_2012$wildAbF + 1) ~  data_2012$temp)
temp_factor <- tidy(lm_temp) %>%
  slice(2) %>%
  select(estimate) %>%
  as_vector()

#Adjusting bee value as though temp is always 20
adjusted_data <- data_2012 %>%
  inner_join(clusterings) %>%
  mutate(adjusted_bees = (((20 - temp) * temp_factor) + log(wildAbF + 1)))


wild_bee_abundance_agglom_adjusted <- adjusted_data %>%
  ggplot(aes(x = temp, y = adjusted_bees)) +
  geom_point(aes(colour = as_factor(node))) +
  geom_smooth() +
  geom_smooth(method = "lm", colour = "red") +
  labs(x = "Temperature", y = "Adjusted Wild Bee Abundance (log(x) + 1)",
       title = "Wild Bee Abundance Coloured by Agglomerature Nodes",
       colour = "Agglomerative clustering nodes") +
  theme_bw() 
wild_bee_abundance_agglom_adjusted 

```

##### X2000nat
Based on previous research undertaken in the white paper it is known that the X2000nat variable has an effect on the the bee count so it also necessary to adjust for this. Wher Again the value will be adjust as though the X2000nat variable is constant. In this case the mean will be chosen as the constant value and the bee count will be adjusted for this in the same way as above. 


```{r}
wild_bee_abundance_agglom_x2000 <- adjusted_data %>%
  ggplot(aes(x = X2000nat, y = adjusted_bees)) +
  geom_point(aes(colour = as_factor(node))) +
  geom_smooth() +
  geom_smooth(method = "lm", colour = "red") +
  labs(x = "X2000nat", y = "Adjusted Wild Bee Abundance (log(x) + 1)",
       title = "Wild Bee Abundance Coloured by Agglomerature Nodes",
       colour = "Agglomerative clustering nodes") +
  theme_bw() 
wild_bee_abundance_agglom_x2000

#Outcome is 0.388
mean_x2000 <- adjusted_data %>%
  summarise(mean_x2000 = mean(X2000nat)) %>%
  as_vector()


lm_X2000nat <- lm(adjusted_data$adjusted_bees ~  adjusted_data$X2000nat)
X2000_factor <- tidy(lm_X2000nat) %>%
  slice(2) %>%
  select(estimate) %>%
  as_vector()

#Adjusting the data for this variable

adjusted_data <- adjusted_data %>%
  mutate(adjusted_bees = ((mean_x2000 - X2000nat) * X2000_factor) + adjusted_bees)

```
So if you look at them individually, it can only be adjusted for one. So need to combine a linear model of all *affecting* factors to best adjust at the end. 

## Analysising other potential confounders
This section will look at finding other parameters to include in the bee variable, dataset used at this point is the original just with the bee variable considered going through a log(x + 1) transformation.
##### Local Diversity
The aim here is to whether adding local diversity as a adjustment factor is going to be beneficial or not. This will be achieved by running a F test to see whether the variances are equal, a shapiro test to confirm the bee data can be assumed to be normal. Note, independence assumption is not validated, this is due more than 1 result being used from each orchard, I decided to include these as I thought the extra power gained from twice the observations was worth the penalty of this assumption.
```{r}
logged_data <- data_2012 %>%
  inner_join(clusterings) %>%
  mutate(adjusted_bees = (((20 - temp) * temp_factor) + log(wildAbF + 1)))

#Testing underlying normal data assumption, again demonstrating why the transformation is neccessary:
#Not normal
shapiro.test(logged_data$wildAbF)
#After transformation -> normal
shapiro.test(logged_data$adjusted_bees)
#Checking if the two subsets have the same variance have the same variance
#Outcome shows that they can be assumed to have the same variance as p.value > 0,05
simple <- logged_data %>% filter(local.diversity == 0) %>% select(wildAbF) %>% as_vector()
diverse <- logged_data %>% filter(local.diversity == 1) %>% select(wildAbF) %>% as_vector()
tidy(var.test(simple, diverse)) %>%
  select(statistic, p.value)
#Performing a two sample t-test on the data to see whether they are the same.
tidy(t.test(wildAbF ~ local.diversity, logged_data, var.equal = TRUE))
#Since H0: Means the same, H1: means are different ~ p.value not significant
#This implies local.diversity does NOT have an effect on the bee counts.






```
## Clustering Summarising
Functions involved with summarising the clusters:
```{r}
#Gets the name of a variable as a string ~ now implemented in clusterise but keeping
get_name <- function(x) {
  deparse(substitute(x))
}

#Gets the bee variables that I am summarising for each cluster - makes it easy to change in future
clusterise <- function(bee_data, cluster_data, v_name = deparse(substitute(cluster_data))){
  inner_join(bee_data, cluster_data) %>%
    summarise(mean_honey_ab = mean(mean_honey_ab),
              mean_wild_ab = mean(mean_wild_ab),
              mean_wild_rich = mean(mean_wild_rich)) %>%
    mutate(cluster = v_name)
}

#The function to output the summaries of both agglom and kmeans clustering
cluster_summarise <- function(bee_data){
output <- list()
  
output$agglom_bees <- clusterise(bee_data, aggl_node1) %>%
  bind_rows(clusterise(bee_data, aggl_node2)) %>%
  bind_rows(clusterise(bee_data, aggl_node3)) %>%
  bind_rows(clusterise(bee_data, aggl_node4)) %>%
  bind_rows(clusterise(bee_data, aggl_node5)) %>%
  bind_rows(clusterise(bee_data, aggl_node6)) %>%
  bind_rows(clusterise(bee_data, aggl_node7)) %>%
  bind_rows(clusterise(bee_data, aggl_node4 %>% filter(cluster == 1), "agglom_node8")) %>%
  bind_rows(clusterise(bee_data, aggl_node4 %>% filter(cluster == 2), "agglom_node9")) %>%
  bind_rows(clusterise(bee_data, aggl_node5 %>% filter(cluster == 1), "agglom_node10")) %>%
  bind_rows(clusterise(bee_data, aggl_node5 %>% filter(cluster == 2), "agglom_node11")) %>%
  bind_rows(clusterise(bee_data, aggl_node6 %>% filter(cluster == 1), "agglom_node12")) %>%
  bind_rows(clusterise(bee_data, aggl_node6 %>% filter(cluster == 2), "agglom_node13")) %>%
  #Only 1 result for cluster as none in the latter cluster
  #Unable to to bind as no common variables for an empty node15
  bind_rows(clusterise(bee_data, aggl_node7 %>% filter(cluster == 1), "agglom_node14")) 


output$kmeans_bees <- clusterise(bee_data, kmeans_node1) %>%
  bind_rows(clusterise(bee_data, kmeans_node2)) %>%
  bind_rows(clusterise(bee_data, kmeans_node3)) %>%
  bind_rows(clusterise(bee_data, kmeans_node4)) %>%
  bind_rows(clusterise(bee_data, kmeans_node5)) %>%
  bind_rows(clusterise(bee_data, kmeans_node6)) %>%
  bind_rows(clusterise(bee_data, kmeans_node7)) %>%
  bind_rows(clusterise(bee_data, kmeans_node4 %>% filter(cluster == 1), "kmeans_node8")) %>%
  bind_rows(clusterise(bee_data, kmeans_node4 %>% filter(cluster == 2), "kmeans_node9")) %>%
  bind_rows(clusterise(bee_data, kmeans_node5 %>% filter(cluster == 1), "kmeans_node10")) %>%
  bind_rows(clusterise(bee_data, kmeans_node5 %>% filter(cluster == 2), "kmeans_node11")) %>%
  bind_rows(clusterise(bee_data, kmeans_node6 %>% filter(cluster == 1), "kmeans_node12")) %>%
  bind_rows(clusterise(bee_data, kmeans_node6 %>% filter(cluster == 2), "kmeans_node13")) %>%
  bind_rows(clusterise(bee_data, kmeans_node7 %>% filter(cluster == 1), "kmeans_node14")) %>%
  bind_rows(clusterise(bee_data, kmeans_node7 %>% filter(cluster == 2), "kmeans_node15"))

#Return a list containing two tibbles one for each type of clustering algorithm
output
}
```

Original summarising without any variable correction
```{r}
#original Bee data
bee_values <- data_2012 %>%
  group_by(orchard) %>%
  summarise(mean_honey_ab = mean(apisAb),
            mean_wild_ab = mean(wildAbF),
            mean_wild_rich = mean(wildRichF)
            )

original_bee_summary <- cluster_summarise(bee_data = bee_values)

#Agglom
original_bee_summary$agglom_bees

#Kmeans
original_bee_summary$kmeans_bees
```
